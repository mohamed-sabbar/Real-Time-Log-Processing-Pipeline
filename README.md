# ğŸš€ Real-Time Log Processing Pipeline

Un pipeline de traitement de logs en temps rÃ©el construit avec **Filebeat**, **Kafka**, **Spark Streaming**, **Flask** et une **interface React** â€” le tout orchestrÃ© avec **Docker**.

---

## ğŸ§­ AperÃ§u de l'architecture

Voici le schÃ©ma du pipeline :

![Architecture](./assets/architecture.png)

### ğŸ”„ Fonctionnement

1. **Filebeat** collecte les logs depuis des fichiers systÃ¨mes ou applicatifs.
2. Les logs sont envoyÃ©s Ã  **Kafka**, qui gÃ¨re les flux de donnÃ©es en temps rÃ©el via des topics.
3. **Spark Streaming** consomme les donnÃ©es Kafka et les traite en temps rÃ©el.
4. Les donnÃ©es traitÃ©es sont renvoyÃ©es Ã  Kafka ou directement exposÃ©es via une **API Flask**.
5. Une **interface React** consomme lâ€™API pour afficher les logs et mÃ©triques en temps rÃ©el.

---

## ğŸ› ï¸ Technologies utilisÃ©es

| Outil         | RÃ´le                                    |
|---------------|-----------------------------------------|
| ğŸ“ Filebeat    | Collecte des fichiers de logs           |
| ğŸ›°ï¸ Kafka       | Broker de messages pour les donnÃ©es      |
| âš¡ Spark       | Traitement de flux en temps rÃ©el        |
| ğŸ Flask       | Exposition des donnÃ©es via une API REST |
| âš›ï¸ React       | Dashboard pour visualiser les donnÃ©es    |
| ğŸ³ Docker      | Conteneurisation et orchestration       |

---

## ğŸ“‚ Structure du projet
```batch 
 Real-Time Log Processing Pipeline:.
â”œâ”€â”€ api/ # API Flask pour l'exposition des donnÃ©es
â”‚ â”œâ”€â”€ api.py # Points d'entrÃ©e de l'API
â”‚ â”œâ”€â”€ requirements.txt # DÃ©pendances Python
â”‚ â””â”€â”€ mon_env/ # Environnement virtuel
â”‚
â”œâ”€â”€ assets/ # Ressources visuelles
â”‚ â”œâ”€â”€ architecture.png # Diagramme d'architecture
â”‚ â””â”€â”€ dashboard.png # Capture du dashboard
â”‚
â”œâ”€â”€ backend/ # Services backend
â”‚ â”œâ”€â”€ kafka/ # Configuration Kafka
â”‚ â”‚ â””â”€â”€ src/ # Code source des producteurs/consommateurs
â”‚ â”‚
â”‚ â””â”€â”€ spark/ # Traitement Spark
â”‚ â””â”€â”€ spark.py # Jobs Spark Streaming
â”‚
â”œâ”€â”€ config/ # Configurations
â”‚ â””â”€â”€ filebeat/
â”‚ â””â”€â”€ filebeat.yml # Configuration Filebeat
â”‚
â”œâ”€â”€ front-end/ # Application React
â”‚ â”œâ”€â”€ public/ # Fichiers statiques
â”‚ â”œâ”€â”€ src/ # Code source React
â”‚ â”œâ”€â”€ .gitignore
â”‚ â”œâ”€â”€ package.json # DÃ©pendances frontend
â”‚ â””â”€â”€ package-lock.json
â”‚
â”œâ”€â”€ .gitignore # Fichiers exclus du versioning
â”œâ”€â”€ docker-compose.yml # Orchestration des containers
â”œâ”€â”€ generated_logs.log # Exemple de logs gÃ©nÃ©rÃ©s
â””â”€â”€ README.md # Ce fichier
```
## ğŸš€ Lancement du projet

1. **Cloner le projet :**

```bash
git clone https://github.com/votre-utilisateur/Big-Data-Project.git
```
2. **Deployer les dependences :**
```bash
cd Real-Time Log Processing Pipeline
docker-compose up
```
3. **Excuter Spark Streaming :**
```bash
docker exec -it spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 /app/spark.py
```
## ğŸ“Š Dashboard
**Lancer le dashboard React :**
```bash
http://localhost:3000
```
