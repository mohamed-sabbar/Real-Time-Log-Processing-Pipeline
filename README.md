# ğŸš€ Real-Time Log Processing Pipeline

Un pipeline de traitement de logs en temps rÃ©el construit avec **Filebeat**, **Kafka**, **Spark Streaming**, **Flask** et une **interface React** â€” le tout orchestrÃ© avec **Docker**.

---

## ğŸ§­ AperÃ§u de l'architecture

Voici le schÃ©ma du pipeline :

![Architecture](./assets/architecture.png)

### ğŸ”„ Fonctionnement

1. **Filebeat** collecte les logs depuis des fichiers systÃ¨mes ou applicatifs.
2. Les logs sont envoyÃ©s Ã  **Kafka**, qui gÃ¨re les flux de donnÃ©es en temps rÃ©el via des topics.
3. **Spark Streaming** consomme les donnÃ©es Kafka et les traite en temps rÃ©el.
4. Les donnÃ©es traitÃ©es sont renvoyÃ©es Ã  Kafka ou directement exposÃ©es via une **API Flask**.
5. Une **interface React** consomme lâ€™API pour afficher les logs et mÃ©triques en temps rÃ©el.

---

## ğŸ› ï¸ Technologies utilisÃ©es

| Outil         | RÃ´le                                    |
|---------------|-----------------------------------------|
| ğŸ“ Filebeat    | Collecte des fichiers de logs           |
| ğŸ›°ï¸ Kafka       | Broker de messages pour les donnÃ©es      |
| âš¡ Spark       | Traitement de flux en temps rÃ©el        |
| ğŸ Flask       | Exposition des donnÃ©es via une API REST |
| âš›ï¸ React       | Dashboard pour visualiser les donnÃ©es    |
| ğŸ³ Docker      | Conteneurisation et orchestration       |

---

## ğŸ“‚ Structure du projet
log-pipeline/ â”‚ 
              â”œâ”€â”€ filebeat/ # Configurations Filebeat 
              â”œâ”€â”€ kafka/ # Docker et configurations Kafka â”œ
              â”€â”€ spark/ # Script Spark Streaming 
              â”œâ”€â”€ flask-api/ # API Flask exposant les rÃ©sultats 
              â”œâ”€â”€ react-dashboard/ # Interface utilisateur React 
              â”œâ”€â”€ docker-compose.yml # Orchestration Docker 
              â””â”€â”€ README.md

